dqn:
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 1024
    buffer_size: 200000
    assign_interval: 1000
    use_priority: False
    n_step: False
    hidden_units: [128, 128]
    
ddqn:
    lr: 5.0e-4
    gamma: 0.99
    epsilon: 0.2
    batch_size: 1024
    buffer_size: 200000
    assign_interval: 1000
    use_priority: False
    n_step: False
    hidden_units: [128, 128]

dddqn:
    lr: 5.0e-4
    gamma: 0.99
    epsilon: 0.2
    batch_size: 1024
    buffer_size: 200000
    assign_interval: 1000
    use_priority: False
    n_step: False
    hidden_units: 
        share: [128]
        v: [128]
        adv: [128]

pg: 
    epsilon: 0.2
    lr: 5.0e-4
    gamma: 0.99
    batch_size: 1024
    epoch: 1  # very important
    hidden_units: 
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
    
ac: 
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    epsilon: 0.2
    gamma: 0.99
    batch_size: 1024
    buffer_size: 200000
    use_priority: False
    n_step: False
    hidden_units: 
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        critic: [64, 64]

a2c: 
    epsilon: 0.2
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    gamma: 0.99
    beta: 1.0e-3
    batch_size: 1024
    epoch: 4  # very important
    hidden_units: 
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        critic: [64, 64]

ppo: 
    share_net: True
    epsilon: 0.2
    gamma: 0.99
    beta: 1.0e-3
    lr: 5.0e-4
    lambda_: 0.97
    batch_size: 128
    epoch: 4  # very important
    actor_lr: 3.0e-4
    critic_lr: 1.0e-3
    hidden_units: 
        share: 
            continuous: 
                share: [64, 64]
                mu: [64, 64]
                v: [64, 64]
            discrete: 
                share: [64, 64]
                logits: [64, 64]
                v: [64, 64]
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        critic: [64, 64]

dpg: 
    gamma: 0.99
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0
    batch_size: 1024
    buffer_size: 200000
    use_priority: False
    n_step: False
    hidden_units: 
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

ddpg: 
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0
    batch_size: 1024
    buffer_size: 200000
    use_priority: False
    n_step: False
    hidden_units: 
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

td3: 
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0 # discrete_tau越小，gumbel采样的越接近one_hot，但相应的梯度也越小
    batch_size: 1024
    buffer_size: 200000
    use_priority: False
    n_step: False
    hidden_units: 
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

sac: 
    alpha: 0.2
    auto_adaption: True
    log_std_bound: [-20, 2]
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    discrete_tau: 1.0
    batch_size: 2048
    buffer_size: 200000
    use_priority: False
    n_step: False
    hidden_units: 
        actor_continuous: 
            share: [128, 128]
            mu: [128]
            log_std: [128]
        actor_discrete: [64, 64]
        q: [128, 128]
        v: [128, 128]

sac_no_v: 
    alpha: 0.2
    auto_adaption: True
    log_std_bound: [-20, 2]
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    gamma: 0.99
    ployak: 0.995
    discrete_tau: 1.0
    batch_size: 1024
    buffer_size: 200000
    use_priority: False
    n_step: False
    hidden_units: 
        actor_continuous: 
            share: [128, 128]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [128, 128]

maxsqn: 
    alpha: 0.2
    beta: 0.1    # 0 <= beta < 1 when beta approaches 1 the distribution of convergence points is closer to uniform distribution means more entropy. when beta approaches 0 the final policy is more deterministic.
    epsilon: 0.2
    use_epsilon: False
    auto_adaption: True
    q_lr: 5.0e-4
    alpha_lr: 5.0e-4
    gamma: 0.999
    ployak: 0.995
    batch_size: 1024
    buffer_size: 200000
    use_priority: False
    n_step: False
    hidden_units: [64, 64]

ma_dpg: 
    gamma: 0.99
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    hidden_units: 
        actor: [64, 64]
        q: [64, 64]

ma_ddpg: 
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    hidden_units: 
        actor: [64, 64]
        q: [64, 64]

ma_td3: 
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    hidden_units: 
        actor: [64, 64]
        q: [64, 64]
